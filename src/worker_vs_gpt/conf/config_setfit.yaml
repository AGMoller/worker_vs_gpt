batch_size: 4
lr_body: 1e-5
lr_head: 1e-2
num_iterations: 1 # Used in the paper (20)
num_epochs_body: 1 # 1 is used in paper
num_epochs_head: 50 # In their tutorial they use 50. Not clear how many they use in training. 
weight_decay: 0
wandb_project: worker_vs_gpt
wandb_entity: cocoons
ckpt: intfloat/e5-base
text_selection: h_text
experiment_type: both # can be 'crowdsourced', 'aug', 'both'
sampling: proportional # can be proportional or balanced
augmentation_model: gpt-3.5-turbo # can be gpt-3.5-turbo or gpt-4